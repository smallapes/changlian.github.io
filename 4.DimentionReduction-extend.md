//  基变换 与 坐标变换

https://zhuanlan.zhihu.com/p/295576029



// svd

https://zhuanlan.zhihu.com/p/57803955

<img src="../../../../../Documents/typora/images/image-20220320192603074.png" alt="image-20220320192603074" style="zoom:50%;" />







//  推荐算法中的矩阵分解 与 svd pca

https://www.sohu.com/a/234584362_129720

|      | svd                   | LSA（Latent Semantic Analysis)&LSI(Latent Semantic Indexing) | pLSA                                | 潜在狄利克雷分布（LDA）<br />Latent Dirichlet Allocation     | 基于深度学习的 lda2vec                                       |
| ---- | --------------------- | ------------------------------------------------------------ | ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|      |                       |                                                              |                                     |                                                              |                                                              |
|      |                       |                                                              | pLSA采取概率方法替代 SVD 以解决问题 | LDA 是 pLSA 的贝叶斯版本。它使用狄利克雷先验来处理文档-主题和单词-主题分布。 | lda2vec 是 word2vec 和 LDA 的扩展，它共同学习单词、文档和主题向量。 |
|      | 矩阵A可以作为转化矩阵 | 词汇表中给出 m 个文档和 n 个单词，我们可以构造一个 m×n 的矩阵 A；<br />文档-术语矩阵 A |                                     |                                                              |                                                              |
|      | 矩阵A被分解           | 使用截断 SVD                                                 | 备注1，EM算法求解                   |                                                              |                                                              |
|      |                       |                                                              |                                     |                                                              |                                                              |





备注1:  P(Z|D) and P(W|Z) are modeled as multinomial distributions, and can be trained using the [expectation-maximization](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm) algorithm (EM).



参考：

https://zhuanlan.zhihu.com/p/151192770

LDA: linear discriminant analysis

lda https://zhuanlan.zhihu.com/p/31470216





//  VC维 与 矩阵分解无关

https://www.zhihu.com/question/23418822

<img src="../../../../../Documents/typora/images/image-20220320215057167.png" alt="image-20220320215057167" style="zoom: 50%;" />



// PCoA的求解过程

https://www.cnblogs.com/lyon2014/p/3991058.html



构成差异矩阵M

构成离差距阵A

求出N个样方的坐标矩阵C

 排列N个样方



// 对角矩阵的各向同性

就是各个位置的特征值都相同





// P	-PCA

![image-20220321132825063](../../../../../Documents/typora/images/image-20220321132825063.png)





// 舒尔补

https://blog.csdn.net/sheagu/article/details/115771184



